# Pi Calculation Service Documentation

## Personal Information
- **Name**: IEONG Kai Yip
- **Student ID**: s13314481

## File Structure and Descriptions
- **s13314481_server.py**: Main server application entry point
- **s13314481_test.py**: Test suite for all components
- **config.py**: Configuration settings
- **requirements.txt**: Package dependencies
- **app/**
  - **controllers/**: API endpoint handlers
  - **models/**: Data models
  - **services/**: Business logic
  - **utils/**: Helper functions
- **tests/**: Test files

## Setup Instructions

### Prerequisites
- Python 3.11.5 (verified working version)
- Virtual environment (recommended)

### Required Third-Party Libraries
```txt
# Core Framework
Flask==3.0.0
Werkzeug==3.1.3
click==8.1.7
itsdangerous==2.2.0
Jinja2==3.1.4
MarkupSafe==3.0.2

# Async Support
aiohappyeyeballs==2.4.4
aiohttp==3.11.9
aiosignal==1.3.1
asyncio==3.4.3

# Testing
pytest==8.3.4
requests==2.32.3

# Additional Dependencies
asgiref==3.8.1
redis==5.2.0
rq==2.0.0
```

### Installation Steps
1. Verify Python version:
```bash
python --version  # Should show Python 3.11.5
```

2. Create virtual environment:
```bash
python -m venv venv
```

3. Activate virtual environment:
```bash
venv\Scripts\activate  # Windows
```

4. Install dependencies:
```bash
pip install -r requirements.txt
```

## Running the Server
1. Ensure virtual environment is activated
2. Start the server:
```bash
python s13314481_server.py
```
3. Server will run on http://localhost:5000

## Running Tests
1. Ensure virtual environment is activated
2. Execute tests:
```bash
python s13314481_test.py
```
### Project Server
- **Host**: localhost
- **Port**: 5000 (TCP)
- **Access URL**: http://localhost:5000

### Legacy Server
- **Host**: localhost
- **TCP Port**: 31416
- **UDP Port**: 31416

## Statistics Web Service Format

### Request Format
```json
{
    "username": "1234",
    "password": "1234-pw"
}
```

### Response Format
```json
[
    {
        "timestamp": "Tue, 03 Dec 2024 16:08:24 GMT",
        "endpoint": "/pi",
        "execution_time": 0.00400233268737793,
        "success": true,
        "parameters": {
            "simulations": 1000,
            "concurrency": 2
        }
    }
]
```

## Concurrency Solutions

### Pi Calculation Service
1. **Asynchronous Task Processing**
   - Uses asyncio for non-blocking operations
   - Handles multiple calculations simultaneously
   - Task queue management for long-running calculations

2. **Thread Pool for Monte Carlo**
   - Parallel processing of simulations
   - Configurable concurrency level (1-8 threads)
   - Efficient resource utilization

### Legacy Pi Service
1. **Protocol Handling**
   - Async TCP/UDP communication
   - Connection pooling for performance
   - Error handling and retries

2. **Request Management**
   - Concurrent request processing
   - Load balancing across protocols
   - Timeout handling

## Advanced Technologies Implementation

### 1. Asynchronous Programming (Unit 12)
- **Implementation**: 
  - Used aiohttp for async HTTP requests
  - asyncio for concurrent operations
  - Non-blocking I/O operations
- **Benefits**:
  - Improved response times
  - Better resource utilization
  - Scalable request handling

### 2. Message Queue System (Unit 11)
- **Implementation**:
  - Redis for task queue
  - RQ for job processing
  - Persistent task storage
- **Benefits**:
  - Reliable task processing
  - System scalability
  - Background job handling

## Additional Notes

### Error Handling
- Comprehensive input validation
- Graceful error recovery
- Detailed error messages

### Security
- Request authentication
- Input sanitization
- Rate limiting

### Testing
- Unit and integration tests
- Concurrent operation testing
- Error scenario coverage

## Advanced Technologies Implementation

### 1. Asynchronous Programming (Unit 12)
The implementation of asynchronous programming would significantly enhance our Pi calculation service by allowing it to handle multiple calculations simultaneously without blocking. Currently, our server processes one calculation at a time, which can lead to delays when multiple users request calculations.

With async implementation:
- Large Pi calculations can be broken into smaller tasks and processed concurrently
- The server can remain responsive while processing calculations
- Users can check calculation progress in real-time
- Resource utilization is improved as the server doesn't wait idle for calculations to complete

For example, when a user requests a Monte Carlo simulation with 1 million points, instead of blocking the server for the entire calculation, the task would be:
1. Split into smaller batches
2. Processed asynchronously
3. Results aggregated while still handling new requests
4. Progress updates sent to clients

### 2. Message Queue System (Unit 11)
Implementing a message queue system would transform our server's ability to handle large calculation requests and manage system resources effectively. Currently, all calculations are processed immediately, which can overwhelm the server during high traffic.

With Redis and RQ implementation:
- Long-running calculations are moved to background processing
- Requests are prioritized based on calculation size
- Server resources are protected from overload
- Failed calculations can be automatically retried
- System stability is improved during peak usage

For instance, when multiple users request Pi calculations:
1. Small calculations (under 10,000 simulations) are processed immediately
2. Large calculations are queued and processed in order
3. Users receive task IDs for tracking progress
4. Server maintains responsiveness regardless of queue size

These improvements would make our server more robust and scalable while maintaining good performance under heavy load.